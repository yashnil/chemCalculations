#!/usr/bin/env python3
# step 4a â†’ finalize.py
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# Fit the Optuna-selected architecture on (train+val),
# evaluate on test, benchmark latency, and save artefacts.
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

import os, time, json, joblib
import numpy as np, pandas as pd, tensorflow as tf
from tensorflow import keras
from sklearn.metrics import r2_score
from losses import composite_loss, _mae_log      # <â€” unchanged

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 0) Paths & load full CSV
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CSV_PATH = "/Users/yashnilmohanty/Desktop/FastChem-Materials/tables/all_gas.csv"
ARTE_DIR = "artefacts"
os.makedirs(ARTE_DIR, exist_ok=True)

df = pd.read_csv(CSV_PATH)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1) Column bookkeeping
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
INPUTS = ["temperature", "pressure",
          "comp_H", "comp_O", "comp_C", "comp_N", "comp_S"]
META    = set(INPUTS) | {"group_index", "point_index"}
SPECIES = [c for c in df.columns if c not in META]
N_OUT   = len(SPECIES)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2) Load splits & scaler
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
spl     = np.load(os.path.join(ARTE_DIR, "splits.npz"), allow_pickle=True)
train_i = spl["train_idx"];  val_i = spl["val_idx"];  test_i = spl["test_idx"]
scaler  = joblib.load(os.path.join(ARTE_DIR, "input_scaler.pkl"))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3) Build scaled X / Y arrays
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
X = df[INPUTS].copy()
X["pressure"] = np.log10(X["pressure"])
for c in INPUTS[2:]:
    X[c] = np.log10(X[c]) + 9.0
X = scaler.transform(X).astype("float32")
Y = df[SPECIES].values.astype("float32")

X_train, Y_train = X[train_i], Y[train_i]
X_val,   Y_val   = X[val_i],   Y[val_i]
X_test,  Y_test  = X[test_i],  Y[test_i]

X_final = np.vstack([X_train, X_val])
Y_final = np.vstack([Y_train, Y_val])

print(f"ðŸ”¹ Final fit on {X_final.shape[0]} samples â€“ test on {X_test.shape[0]} samples")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4) Reload Optuna best hyper-parameters
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
study = joblib.load(os.path.join(ARTE_DIR, "optuna_study.pkl"))
best  = study.best_params
print("Best hyper-parameters:", best)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5) Rebuild the model (identical to tune.py)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
keras.backend.clear_session()
model = keras.Sequential(name="surrogate_final")
model.add(keras.layers.Input(shape=(X.shape[1],)))
for _ in range(best["n_layers"]):
    model.add(keras.layers.Dense(best["units"], activation=best["act"]))
model.add(keras.layers.Dense(N_OUT, activation="softmax"))

# â”€â”€ Loss: keep Î² = 0 so NO mass-balance term â”€â”€
model.compile(
    optimizer=keras.optimizers.Adam(best["lr"]),
    loss=composite_loss(lam=best["lam"], beta=0.0),
    metrics=[_mae_log],           # MAE (log-space) only
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6) Train on train+val  (monitor val MAE_log)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
callbacks = [
    keras.callbacks.EarlyStopping(
        monitor="val__mae_log",   # <â€” keep using the metric you care about
        mode="min",               # <â€” tell Keras â€œsmaller is betterâ€
        patience=20,
        restore_best_weights=True
    ),
    keras.callbacks.ReduceLROnPlateau(
        monitor="val__mae_log",
        mode="min",               # <â€” same here
        factor=0.5,
        patience=5,
        min_lr=1e-6
    ),
]

t0 = time.time()
model.fit(
    X_train, Y_train,
    validation_data=(X_val, Y_val),
    epochs=200,
    batch_size=128,
    verbose=2,
    callbacks=callbacks,
)
print(f"â±  Final fit: {(time.time()-t0)/60:.1f} min")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7) Test-set metrics
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
pred   = model.predict(X_test, batch_size=256, verbose=0)
mae_lg = _mae_log(tf.constant(Y_test), tf.constant(pred)).numpy()
r2     = r2_score(Y_test, pred, multioutput="variance_weighted")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 8) Latency benchmark
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FASTCHEM_MS = 6.13
idx   = np.random.choice(len(X_test), 1000, replace=True)
x_b   = X_test[idx]
_     = model.predict(x_b[:16], batch_size=16, verbose=0)   # warm-up
t0    = time.time(); model.predict(x_b, batch_size=256, verbose=0)
nn_ms = (time.time()-t0)/1000 * 1e3
speed = FASTCHEM_MS / nn_ms

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 9) Save artefacts
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model.save(os.path.join(ARTE_DIR, "final_model.keras"))
with open(os.path.join(ARTE_DIR, "final_report.json"), "w") as f:
    json.dump({
        "mae_log_test":  float(mae_lg),
        "r2_test":       float(r2),
        "latency_ms":    nn_ms,
        "speedup":       speed,
        "hyperparams":   best,
        "species":       SPECIES,
    }, f, indent=2)

print("\n   FINAL TEST")
print(f"   MAE_log       : {mae_lg:10.4e}")
print(f"   weighted RÂ²   : {r2:10.3f}")
print(f"   latency       : {nn_ms:10.3f} ms / pt   (Ã—{speed:.1f} vs FastChem)")
print("âœ”ï¸  Artefacts saved â†’", ARTE_DIR)
